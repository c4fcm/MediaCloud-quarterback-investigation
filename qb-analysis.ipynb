{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodecsv as csv\n",
    "from PyDictionary import PyDictionary\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open manually coded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name - 50 terms\n",
      "personal - 14 terms\n",
      "stopword - 115 terms\n",
      "descriptor - 25 terms\n",
      "health - 16 terms\n",
      "action - 29 terms\n"
     ]
    }
   ],
   "source": [
    "# manual-word-tags.csv contains 249 most common words in corpus,\n",
    "# manually sorted into \"name\", \"personal\", \"stopword\", \"descriptor\", \"health\", and \"action\"\n",
    "# here we import them into a dictionary, \"codings\"\n",
    "\n",
    "codings = {}\n",
    "reader = csv.reader( open('manual-word-tags.csv','rU'))\n",
    "reader.next()\n",
    "for row in reader:\n",
    "    word = row[0]\n",
    "    tag = row[1]\n",
    "    if tag not in codings:\n",
    "        codings[tag] = []\n",
    "    codings[tag].append(word)\n",
    "for category,terms in codings.iteritems():\n",
    "    print \"%s - %d terms\" % (category, len(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal - 73 terms\n",
      "descriptor - 119 terms\n",
      "health - 73 terms\n",
      "action - 132 terms\n",
      "No repeats!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allan\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# use PyDictionary to broaden word categories by getting synonyms of all words in each category\n",
    "\n",
    "to_expand = ['personal','descriptor','health','action']\n",
    "expanded_codings = {}\n",
    "\n",
    "for category in to_expand:\n",
    "    expanded_codings[category] = codings[category][:]\n",
    "    \n",
    "for category in to_expand:\n",
    "    \n",
    "    dictionary = PyDictionary(expanded_codings[category])\n",
    "    synonymlists = dictionary.getSynonyms(formatted=False)\n",
    "    \n",
    "    #\"synonymlists\" is a list of lists. each list is synonyms of a word in \"terms\"\n",
    "    #dictionary.getSynonyms(formatted = True) would return a list of dicts\n",
    "    \n",
    "    synonyms = [word for sublist in synonymlists for word in sublist ] #flatten list of lists into a list\n",
    "    synonyms = list(set(synonyms)) #remove duplicates\n",
    "    \n",
    "    #remove all words that are already in current list of words\n",
    "    #note that this biases word tags: if a new word is tagged both \"personal\" and \"health\", the code decides\n",
    "    #to only tag it with \"personal\" and remove it from \"health\"\n",
    "    allwords = expanded_codings.values()\n",
    "    allwords = [word for sublist in allwords for word in sublist]\n",
    "    \n",
    "    #if the word is already tagged something else in \"codings\", don't include it here.\n",
    "    allcodingswords = codings.values()\n",
    "    allcodingswords = [word for sublist in allcodingswords for word in sublist]\n",
    "    \n",
    "    #if the word is in our precompiled list of stopwords (stop-words-english4.txt), don't include it here\n",
    "    with open('stop-words-english4.txt', 'r') as f:\n",
    "        stopwords = f.read().split()\n",
    "    \n",
    "    #remove all undesired words\n",
    "    synonyms = [word for word in synonyms if word not in allwords + allcodingswords + stopwords]\n",
    "    \n",
    "    expanded_codings[category] += synonyms\n",
    "    \n",
    "for category,terms in expanded_codings.iteritems():\n",
    "    print \"%s - %d terms\" % (category, len(terms))\n",
    "    \n",
    "# write to a CSV \"expanded-word-tags.csv\"\n",
    "with open('expanded-word-tags.csv', 'w') as f:\n",
    "    f.write('word,tag\\n')\n",
    "    \n",
    "    for category in to_expand:\n",
    "        for s in expanded_codings[category]:\n",
    "            towrite = s + ',' + category + '\\n'\n",
    "            f.write(towrite)\n",
    "\n",
    "# check if there are any words in multiple categories\n",
    "allwords = expanded_codings.values()\n",
    "allwords = [word for sublist in allwords for word in sublist] #flatten list of lists to a 1-level list\n",
    "repeats = [word for word in allwords if allwords.count(word) > 1]\n",
    "repeats = list(set(repeats))\n",
    "if len(repeats) == 0:\n",
    "    print \"No repeats!\"\n",
    "else:\n",
    "    print \"Repeated words:\", repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal - 71 terms\n",
      "stopword - 119 terms\n",
      "health - 46 terms\n",
      "action - 68 terms\n",
      "descriptor - 93 terms\n"
     ]
    }
   ],
   "source": [
    "# we want to manually check our new word tags: this is done in \"expanded-word-tags-manual-edit.csv\"\n",
    "# so we re-import the words into a new dictionary, expanded_manual_codings:\n",
    "\n",
    "expanded_manual_codings = {}\n",
    "reader = csv.reader( open('expanded-word-tags-manual-edit.csv','rU'))\n",
    "reader.next()\n",
    "for row in reader:\n",
    "    word = row[0]\n",
    "    tag = row[1]\n",
    "    if tag not in expanded_manual_codings:\n",
    "        expanded_manual_codings[tag] = []\n",
    "    expanded_manual_codings[tag].append(word)\n",
    "for category,terms in expanded_manual_codings.iteritems():\n",
    "    print \"%s - %d terms\" % (category, len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/word_freq_by_race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top200white = df.sort_values('white',ascending=False).head(200)['word']\n",
    "#for some reason, #24048 gets put at the top?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top200other = df.sort_values('other',ascending=False).head(200)['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top200other[~top200other.isin(top200white)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Category Use by Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'action': {'other': 0.03791378, 'white': 0.038188879999999994},\n",
       " u'descriptor': {'other': 0.040160762, 'white': 0.04241355999999999},\n",
       " u'health': {'other': 0.028549151, 'white': 0.023718885999999995},\n",
       " u'name': {'other': 0.046217126, 'white': 0.038795537},\n",
       " u'personal': {'other': 0.019783949, 'white': 0.016239604999999997},\n",
       " u'stopword': {'other': 0.3200348539999997, 'white': 0.3064274950000001}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using codings, no synonyms\n",
    "uses = {}\n",
    "for category,terms in codings.iteritems():\n",
    "    white_uses = 0\n",
    "    other_uses = 0\n",
    "    uses[category] = {'white':df[df['word'].isin(terms)].sum(axis=0)['white pct'],\n",
    "                      'other':df[df['word'].isin(terms)].sum(axis=0)['other pct']}\n",
    "uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': {'other': 0.04337066300000003, 'white': 0.04362302499999999},\n",
       " 'descriptor': {'other': 0.04620107200000006, 'white': 0.047628353000000005},\n",
       " 'health': {'other': 0.030651998, 'white': 0.025786419999999984},\n",
       " 'personal': {'other': 0.023033549000000014, 'white': 0.020322015999999988}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using expanded_codings, with synonyms\n",
    "uses = {}\n",
    "for category,terms in expanded_codings.iteritems():\n",
    "    white_uses = 0\n",
    "    other_uses = 0\n",
    "    uses[category] = {'white':df[df['word'].isin(terms)].sum(axis=0)['white pct'],\n",
    "                      'other':df[df['word'].isin(terms)].sum(axis=0)['other pct']}\n",
    "uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'action': {'other': 0.04104851500000001, 'white': 0.04140602299999999},\n",
       " u'descriptor': {'other': 0.04442236800000004, 'white': 0.046455509},\n",
       " u'health': {'other': 0.030743599, 'white': 0.025870138999999986},\n",
       " u'personal': {'other': 0.022587996000000013, 'white': 0.019858655999999985},\n",
       " u'stopword': {'other': 0.004454803999999995, 'white': 0.003769487}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using expanded_manual_codings\n",
    "uses = {}\n",
    "for category,terms in expanded_manual_codings.iteritems():\n",
    "    white_uses = 0\n",
    "    other_uses = 0\n",
    "    uses[category] = {'white':df[df['word'].isin(terms)].sum(axis=0)['white pct'],\n",
    "                      'other':df[df['word'].isin(terms)].sum(axis=0)['other pct']}\n",
    "uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Count number of times each qb is mentioned in their corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need a proxy for measuring \"how athletic/good each qb is\"\n",
    "# \"rushing yards\" is sometimes used - plot rushing yards for each quarterback, color by white/nonwhite\n",
    "# also see if you can find a source for forty-yard dash times - make the same chart as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count number of times each qb is mentioned (full name) in their own .txt file of sentences\n",
    "\n",
    "#TODO: give me a bar chart of number of sentences per quarterback\n",
    "#and also a chart white vs other, number of sentences divided by # of quarterbacks in each category\n",
    "\n",
    "qbfiles = listdir(r'data\\corpora')\n",
    "names = [name[:-4] for name in qbfiles]\n",
    "\n",
    "counts = {}\n",
    "\n",
    "for n in names:\n",
    "    filename = 'data\\\\corpora\\\\'+n+'.txt'\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        alltext = f.read()\n",
    "    \n",
    "    counts[n] = alltext.count(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get races of each quarterback according to qb_table\n",
    "races = pd.read_csv('qb-table.csv', index_col=2)\n",
    "allraces = list(set(races.index))\n",
    "\n",
    "whiteqbs = list(races.loc['white']['qb_name'])\n",
    "\n",
    "nonwhiteqbs = []\n",
    "for race in allraces:\n",
    "    if race != 'white':\n",
    "        addnames = races.loc[race]['qb_name']\n",
    "        nonwhiteqbs += [addnames]\n",
    "\n",
    "#nonwhiteqbs = [list(races.loc[race]['qb_name']) for race in allraces if race != 'white']\n",
    "nonwhiteqbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
